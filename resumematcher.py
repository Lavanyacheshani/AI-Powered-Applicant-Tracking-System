# -*- coding: utf-8 -*-
"""ResumeMatcher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kIyrD9xYcnC9baoa6GnSO66v632IEUCm
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install -q kaggle

# Download the dataset
!kaggle datasets download -d snehaanbhawal/resume-dataset

!unzip -q resume-dataset.zip

!ls

import pandas as pd

df = pd.read_csv('/content/Resume/Resume.csv')
df.head()

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\b\d+\b', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['Cleaned_Resume'] = df['Resume_str'].apply(clean_text)

job_descriptions = [
    "Looking for a data scientist with experience in Python, ML, and data analysis.",
    "We need a web developer skilled in HTML, CSS, JavaScript, and React.",
    "Hiring a network engineer with strong knowledge of routing and security."
]

# Clean them
jd_clean = [clean_text(jd) for jd in job_descriptions]

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Combine all resumes and job descriptions for TF-IDF vectorization
combined_texts = df['Cleaned_Resume'].tolist() + jd_clean

# Vectorize
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(combined_texts)

# Separate vectors
resume_vectors = tfidf_matrix[:len(df)]
jd_vectors = tfidf_matrix[len(df):]

# Each JD compared with all resumes
for i, jd_vec in enumerate(jd_vectors):
    similarity_scores = cosine_similarity(jd_vec, resume_vectors).flatten()
    top_matches = similarity_scores.argsort()[::-1][:5]  # top 5 matches

    print(f"\nüìÑ Top resumes for JD #{i+1}: {job_descriptions[i]}\n")
    for idx in top_matches:
        print(f"‚úÖ Resume #{idx} - Category: {df.iloc[idx]['Category']}")
        print(df.iloc[idx]['Resume_str'][:300], "...\n")  # print a short snippet

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer, util
import pandas as pd

# Load the BERT-based model
model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast & good for semantic search

df = pd.read_csv('/content/Resume/Resume.csv')

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\b\d+\b', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['Cleaned_Resume'] = df['Resume_str'].apply(clean_text)

resume_embeddings = model.encode(df['Cleaned_Resume'].tolist(), convert_to_tensor=True)

job_description = """
We are hiring a data scientist skilled in Python, pandas, machine learning, and model evaluation.
Experience in NLP and visualization tools like matplotlib or seaborn is preferred.
"""

cleaned_jd = clean_text(job_description)
jd_embedding = model.encode(cleaned_jd, convert_to_tensor=True)

import torch

# Compute cosine similarities
cosine_scores = util.pytorch_cos_sim(jd_embedding, resume_embeddings)[0]

# Get Top 5 Matches
top_results = torch.topk(cosine_scores, k=5)

print("üîç Job Description:")
print(job_description)

print("\n‚úÖ Top 5 Matching Resumes:\n")
for score, idx in zip(top_results[0], top_results[1]):
    print(f"Score: {score.item():.4f} | Category: {df.iloc[idx.item()]['Category']}")
    print(df.iloc[idx.item()]['Resume_str'][:300], '...\n')  # First 300 characters

df.to_csv("CleanedResumeDataSet.csv", index=False)

with open("requirements.txt", "w") as f:
    f.write("pandas\nsentence-transformers\ntorch\nscikit-learn\n")